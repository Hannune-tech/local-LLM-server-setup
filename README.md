# local-LLM-server-setup

## Overview

This repository provides a comprehensive guide and setup scripts for configuring a personal server to run local Large Language Models (LLMs) and utilize other features like vector databases. The setup is designed to work with Docker containers, offering flexibility and ease of deployment.

## Motivation

The primary motivation behind this project is to enable individuals to set up their own server environment for running open-source LLMs locally. This setup allows for:

- Running various open-source LLMs (including Deepseek r1 models)
- Utilizing frameworks like LangChain and LlamaIndex
- Deploying multiple AI-related applications
- Hosting applications on a personal website

By following this guide, you can create a powerful, customizable AI development environment on your own hardware.

## Prerequisites

Before using this repository, ensure you have the following:

1. A server with a Linux operating system (Ubuntu 22.04 recommended)
2. Internet connection
3. Properly configured router for network access
4. Basic knowledge of Linux command line and Docker

## Features

- Server setup scripts
- Docker configuration for containerized applications
- GPU support for NVIDIA and AMD graphics cards
- Deploy a local LLM server with open source LLMs
- Vector database setup for efficient data storage and retrieval

## Directory Structure

- `setup/`: Server setup scripts
- `docker/`: Docker configuration files
- `models/`: Guides for setting up specific LLMs and AI tools
- `tools/`: Configuration guides for additional development tools

## Contributing

Contributions to improve the setup process or add support for new models/tools are welcome. Please feel free to submit pull requests or open issues for any bugs or feature requests.

## License

This project is licensed under the [MIT License](LICENSE).

## Acknowledgements

This project was inspired by the growing need for accessible, local AI infrastructure and the incredible work of the open-source AI community.


